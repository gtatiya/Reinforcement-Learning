{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import gym_novel_gridworlds\n",
    "import numpy as np\n",
    "\n",
    "# %matplotlib inline\n",
    "# %matplotlib nbagg\n",
    "\n",
    "# pip install PyQt5\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space: Discrete(3)\n",
      "observation_space: Box(10,)\n",
      "observation_space.low: [1 1 1 1 1 1 1 1 1 1]\n",
      "observation_space.high: [11 11 11 11 11 11 11 11 11 11]\n",
      "sample: [ 5  5  7  8  4  5 10  1  5  6] 1\n"
     ]
    }
   ],
   "source": [
    "env_id = 'NovelGridworld-v0'\n",
    "\n",
    "env = gym.make(env_id)\n",
    "print(\"action_space:\", env.action_space)\n",
    "print(\"observation_space:\", env.observation_space)\n",
    "print(\"observation_space.low:\", env.observation_space.low)\n",
    "print(\"observation_space.high:\", env.observation_space.high)\n",
    "print(\"sample:\", env.observation_space.sample(), env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  0 Forward\n",
      "Step: 0, reward:  -1\n",
      "observation:  [11  6 11  8  4 11 11  4 11  3]\n",
      "action:  0 Forward\n",
      "Step: 1, reward:  -1\n",
      "observation:  [11  6 11  7  3 11 11  4 11  3]\n",
      "action:  1 Left\n",
      "Step: 2, reward:  -1\n",
      "observation:  [ 3 11 11  4 11  3 11  4 11  4]\n",
      "action:  1 Left\n",
      "Step: 3, reward:  -1\n",
      "observation:  [11  3 11  4 11  4 11  5 11  6]\n",
      "action:  0 Forward\n",
      "Step: 4, reward:  -1\n",
      "observation:  [11  3 11  4 11  3 11  4 11  6]\n",
      "action:  1 Left\n",
      "Step: 5, reward:  -1\n",
      "observation:  [11  3 11  4 11  6 11  8  4 11]\n",
      "action:  0 Forward\n",
      "Step: 6, reward:  -1\n",
      "observation:  [11  3 11  4 11  5 11  7 11  6]\n",
      "action:  0 Forward\n",
      "Step: 7, reward:  -1\n",
      "observation:  [11  3 11  4 11  4 11  5 11  6]\n",
      "action:  1 Left\n",
      "Step: 8, reward:  -1\n",
      "observation:  [11  4 11  5 11  6 11  7 11  5]\n",
      "action:  2 Right\n",
      "Step: 9, reward:  -1\n",
      "observation:  [11  3 11  4 11  4 11  5 11  6]\n",
      "action:  0 Forward\n",
      "Step: 10, reward:  -1\n",
      "observation:  [11  3 11  4 11  3 11  4 11  6]\n",
      "action:  2 Right\n",
      "Step: 11, reward:  -1\n",
      "observation:  [11  6 11  4 11  3 11  4 11  3]\n",
      "action:  0 Forward\n",
      "Step: 12, reward:  -1\n",
      "observation:  [11  6 11  3 11  2 11  3 11  3]\n",
      "action:  2 Right\n",
      "Step: 13, reward:  -1\n",
      "observation:  [11  7 11  8 11  6 11  3 11  2]\n",
      "action:  1 Left\n",
      "Step: 14, reward:  -1\n",
      "observation:  [11  6 11  3 11  2 11  3 11  3]\n",
      "action:  0 Forward\n",
      "Step: 15, reward:  -1\n",
      "observation:  [11  6 11  1 11  1 11  1 11  3]\n",
      "action:  1 Left\n",
      "Step: 16, reward:  -1\n",
      "observation:  [11  1 11  1 11  3 11  4 11  8]\n",
      "action:  0 Forward\n",
      "Step: 17, reward:  -1\n",
      "observation:  [11  1 11  1 11  2 11  3 11  8]\n",
      "action:  2 Right\n",
      "Step: 18, reward:  -1\n",
      "observation:  [11  7 11  1 11  1 11  1 11  2]\n",
      "action:  0 Forward\n",
      "Step: 19, reward:  -1\n",
      "observation:  [11  7 11  1 11  1 11  1 11  2]\n",
      "action:  2 Right\n",
      "Step: 20, reward:  -1\n",
      "observation:  [11  8 11 10 11  7 11  1 11  1]\n",
      "action:  0 Forward\n",
      "Step: 21, reward:  -1\n",
      "observation:  [11  8 11  8 11  6 11  1 11  1]\n",
      "action:  1 Left\n",
      "Step: 22, reward:  -1\n",
      "observation:  [11  6 11  1 11  1 11  1 11  3]\n",
      "action:  2 Right\n",
      "Step: 23, reward:  -1\n",
      "observation:  [11  8 11  8 11  6 11  1 11  1]\n",
      "action:  1 Left\n",
      "Step: 24, reward:  -1\n",
      "observation:  [11  6 11  1 11  1 11  1 11  3]\n",
      "action:  2 Right\n",
      "Step: 25, reward:  -1\n",
      "observation:  [11  8 11  8 11  6 11  1 11  1]\n",
      "action:  0 Forward\n",
      "Step: 26, reward:  -1\n",
      "observation:  [11  8 11  7 11  5 11  1 11  1]\n",
      "action:  0 Forward\n",
      "Step: 27, reward:  -1\n",
      "observation:  [11  8 11  5 11  4 11  1 11  1]\n",
      "action:  0 Forward\n",
      "Step: 28, reward:  -1\n",
      "observation:  [ 6 11 11  4 11  3 11  1 11  1]\n",
      "action:  1 Left\n",
      "Step: 29, reward:  -1\n",
      "observation:  [11  3 11  1 11  1 11  1 11  6]\n",
      "action:  2 Right\n",
      "Step: 30, reward:  -1\n",
      "observation:  [ 6 11 11  4 11  3 11  1 11  1]\n",
      "action:  0 Forward\n",
      "Step: 31, reward:  -1\n",
      "observation:  [11  8 11  3 11  2 11  1 11  1]\n",
      "action:  1 Left\n",
      "Step: 32, reward:  -1\n",
      "observation:  [11  2 11  1 11  1 11  1 11  7]\n",
      "action:  2 Right\n",
      "Step: 33, reward:  -1\n",
      "observation:  [11  8 11  3 11  2 11  1 11  1]\n",
      "action:  1 Left\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "for i in range(50):\n",
    "    action = env.action_space.sample()  # take a random action\n",
    "    print(\"action: \", action, env.action_str[action])\n",
    "    # print(\"agent_location: \", env.agent_location)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    print(\"Step: \" + str(i) + \", reward: \", reward)\n",
    "    print(\"observation: \", observation)\n",
    "    time.sleep(0)\n",
    "\n",
    "#     if (i+1) % 10 == 0:\n",
    "#         env.map_size = np.random.randint(low=10, high=20, size=1)[0]\n",
    "#         obs = env.reset()\n",
    "#         print(\"\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\gyant\\documents\\github\\stable-baselines\\stable_baselines\\common\\tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\gyant\\documents\\github\\stable-baselines\\stable_baselines\\common\\tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\gyant\\documents\\github\\stable-baselines\\stable_baselines\\deepq\\dqn.py:129: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\gyant\\documents\\github\\stable-baselines\\stable_baselines\\deepq\\build_graph.py:361: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\gyant\\documents\\github\\stable-baselines\\stable_baselines\\deepq\\build_graph.py:362: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\gyant\\documents\\github\\stable-baselines\\stable_baselines\\deepq\\build_graph.py:139: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\gyant\\documents\\github\\stable-baselines\\stable_baselines\\deepq\\policies.py:109: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CA73250C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CA73250C8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CA73250C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CA73250C8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA6D2F508>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA6D2F508>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA6D2F508>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA6D2F508>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA6D1FA48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA6D1FA48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA6D1FA48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA6D1FA48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA7335948>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA7335948>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA7335948>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA7335948>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA7356648>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA7356648>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA7356648>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA7356648>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA6D1FF48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA6D1FF48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA6D1FF48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA6D1FF48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA7325988>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA7325988>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA7325988>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA7325988>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From c:\\users\\gyant\\documents\\github\\stable-baselines\\stable_baselines\\deepq\\build_graph.py:151: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CBE341D88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CBE341D88>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CBE341D88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CBE341D88>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE348348>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE348348>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE348348>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE348348>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE31EFC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE31EFC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE31EFC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE31EFC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE223D48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE223D48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE223D48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE223D48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE1E6AC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE1E6AC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE1E6AC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE1E6AC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE213848>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE213848>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE213848>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE213848>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA7381788>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA7381788>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA7381788>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA7381788>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CBE3410C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CBE3410C8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CBE3410C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CBE3410C8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE31EB08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE31EB08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE31EB08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE31EB08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE3410C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE3410C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE3410C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE3410C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9DAAF48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9DAAF48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9DAAF48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9DAAF48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA737BA48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA737BA48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA737BA48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA737BA48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA737BA48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA737BA48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA737BA48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA737BA48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE223848>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE223848>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE223848>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE223848>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CC9E6CEC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CC9E6CEC8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CC9E6CEC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CC9E6CEC8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE1B8548>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE1B8548>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE1B8548>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CBE1B8548>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9D681C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9D681C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9D681C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9D681C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9D681C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9D681C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9D681C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9D681C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9D79388>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9D79388>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9D79388>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9D79388>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9D79388>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9D79388>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9D79388>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9D79388>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9D681C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9D681C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9D681C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9D681C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "model.q_values:  Tensor(\"deepq/model/add:0\", shape=(?, 3), dtype=float32)\n",
      "step_model.q_values:  Tensor(\"deepq/step_model/model/add:0\", shape=(?, 3), dtype=float32)\n",
      "double_policy.q_values:  Tensor(\"deepq/double_q/model/add:0\", shape=(?, 3), dtype=float32)\n",
      "target_policy.q_values:  Tensor(\"deepq/target_q_func/model/add:0\", shape=(?, 3), dtype=float32)\n",
      "WARNING:tensorflow:From c:\\users\\gyant\\documents\\github\\stable-baselines\\stable_baselines\\deepq\\build_graph.py:427: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "q_func_vars:  [<tf.Variable 'deepq/model/action_value/fully_connected/weights:0' shape=(10, 64) dtype=float32_ref>, <tf.Variable 'deepq/model/action_value/fully_connected/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'deepq/model/action_value/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref>, <tf.Variable 'deepq/model/action_value/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'deepq/model/action_value/fully_connected_2/weights:0' shape=(64, 3) dtype=float32_ref>, <tf.Variable 'deepq/model/action_value/fully_connected_2/biases:0' shape=(3,) dtype=float32_ref>, <tf.Variable 'deepq/model/state_value/fully_connected/weights:0' shape=(10, 64) dtype=float32_ref>, <tf.Variable 'deepq/model/state_value/fully_connected/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'deepq/model/state_value/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref>, <tf.Variable 'deepq/model/state_value/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'deepq/model/state_value/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref>, <tf.Variable 'deepq/model/state_value/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref>]\n",
      "target_q_func_vars:  [<tf.Variable 'deepq/target_q_func/model/action_value/fully_connected/weights:0' shape=(10, 64) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/action_value/fully_connected/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/action_value/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/action_value/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/action_value/fully_connected_2/weights:0' shape=(64, 3) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/action_value/fully_connected_2/biases:0' shape=(3,) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/state_value/fully_connected/weights:0' shape=(10, 64) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/state_value/fully_connected/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/state_value/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/state_value/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/state_value/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/state_value/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref>]\n",
      "WARNING:tensorflow:From c:\\users\\gyant\\documents\\github\\stable-baselines\\stable_baselines\\deepq\\build_graph.py:463: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 48       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | -2.4     |\n",
      "| steps                   | 5286     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 34       |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 37.2     |\n",
      "| steps                   | 6669     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 19       |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 35.5     |\n",
      "| steps                   | 8223     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 17.8     |\n",
      "| steps                   | 11539    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | 39.1     |\n",
      "| steps                   | 12727    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | 41.8     |\n",
      "| steps                   | 13651    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | 44.4     |\n",
      "| steps                   | 14311    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | 44.6     |\n",
      "| steps                   | 14947    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 900      |\n",
      "| mean 100 episode reward | 43.7     |\n",
      "| steps                   | 15681    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1000     |\n",
      "| mean 100 episode reward | 44.2     |\n",
      "| steps                   | 16363    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1100     |\n",
      "| mean 100 episode reward | 43.6     |\n",
      "| steps                   | 17103    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1200     |\n",
      "| mean 100 episode reward | 43       |\n",
      "| steps                   | 17898    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1300     |\n",
      "| mean 100 episode reward | 43.7     |\n",
      "| steps                   | 18631    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1400     |\n",
      "| mean 100 episode reward | 43.5     |\n",
      "| steps                   | 19383    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1500     |\n",
      "| mean 100 episode reward | 44.8     |\n",
      "| steps                   | 20000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1600     |\n",
      "| mean 100 episode reward | 43.1     |\n",
      "| steps                   | 20793    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1700     |\n",
      "| mean 100 episode reward | 44.3     |\n",
      "| steps                   | 21465    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1800     |\n",
      "| mean 100 episode reward | 44       |\n",
      "| steps                   | 22167    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1900     |\n",
      "| mean 100 episode reward | 42.7     |\n",
      "| steps                   | 22995    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2000     |\n",
      "| mean 100 episode reward | 43.5     |\n",
      "| steps                   | 23745    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2100     |\n",
      "| mean 100 episode reward | 43.8     |\n",
      "| steps                   | 24466    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2200     |\n",
      "| mean 100 episode reward | 42.7     |\n",
      "| steps                   | 25296    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2300     |\n",
      "| mean 100 episode reward | 44       |\n",
      "| steps                   | 25991    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2400     |\n",
      "| mean 100 episode reward | 44       |\n",
      "| steps                   | 26686    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2500     |\n",
      "| mean 100 episode reward | 42.9     |\n",
      "| steps                   | 27495    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2600     |\n",
      "| mean 100 episode reward | 43.1     |\n",
      "| steps                   | 28282    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2700     |\n",
      "| mean 100 episode reward | 42.6     |\n",
      "| steps                   | 29126    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2800     |\n",
      "| mean 100 episode reward | 42.9     |\n",
      "| steps                   | 29937    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2900     |\n",
      "| mean 100 episode reward | 42.2     |\n",
      "| steps                   | 30818    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3000     |\n",
      "| mean 100 episode reward | 43.2     |\n",
      "| steps                   | 31593    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3100     |\n",
      "| mean 100 episode reward | 43       |\n",
      "| steps                   | 32391    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3200     |\n",
      "| mean 100 episode reward | 42.6     |\n",
      "| steps                   | 33229    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3300     |\n",
      "| mean 100 episode reward | 41.7     |\n",
      "| steps                   | 34156    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3400     |\n",
      "| mean 100 episode reward | 42.6     |\n",
      "| steps                   | 34995    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3500     |\n",
      "| mean 100 episode reward | 42       |\n",
      "| steps                   | 35893    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3600     |\n",
      "| mean 100 episode reward | 39.9     |\n",
      "| steps                   | 37007    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3700     |\n",
      "| mean 100 episode reward | 41.5     |\n",
      "| steps                   | 37958    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3800     |\n",
      "| mean 100 episode reward | 43.2     |\n",
      "| steps                   | 38735    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3900     |\n",
      "| mean 100 episode reward | 42.4     |\n",
      "| steps                   | 39597    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4000     |\n",
      "| mean 100 episode reward | 42.7     |\n",
      "| steps                   | 40425    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4100     |\n",
      "| mean 100 episode reward | 42.7     |\n",
      "| steps                   | 41259    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4200     |\n",
      "| mean 100 episode reward | 43.5     |\n",
      "| steps                   | 42014    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4300     |\n",
      "| mean 100 episode reward | 42.3     |\n",
      "| steps                   | 42884    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4400     |\n",
      "| mean 100 episode reward | 43.5     |\n",
      "| steps                   | 43632    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4500     |\n",
      "| mean 100 episode reward | 43       |\n",
      "| steps                   | 44437    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4600     |\n",
      "| mean 100 episode reward | 43.1     |\n",
      "| steps                   | 45230    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4700     |\n",
      "| mean 100 episode reward | 44       |\n",
      "| steps                   | 45933    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4800     |\n",
      "| mean 100 episode reward | 42.1     |\n",
      "| steps                   | 46825    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4900     |\n",
      "| mean 100 episode reward | 42.5     |\n",
      "| steps                   | 47680    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5000     |\n",
      "| mean 100 episode reward | 41.4     |\n",
      "| steps                   | 48639    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5100     |\n",
      "| mean 100 episode reward | 41.8     |\n",
      "| steps                   | 49559    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5200     |\n",
      "| mean 100 episode reward | 43.5     |\n",
      "| steps                   | 50313    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5300     |\n",
      "| mean 100 episode reward | 42.3     |\n",
      "| steps                   | 51181    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5400     |\n",
      "| mean 100 episode reward | 42.7     |\n",
      "| steps                   | 52012    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5500     |\n",
      "| mean 100 episode reward | 40.9     |\n",
      "| steps                   | 53026    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5600     |\n",
      "| mean 100 episode reward | 42.9     |\n",
      "| steps                   | 53832    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5700     |\n",
      "| mean 100 episode reward | 41.5     |\n",
      "| steps                   | 54780    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5800     |\n",
      "| mean 100 episode reward | 40.5     |\n",
      "| steps                   | 55826    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5900     |\n",
      "| mean 100 episode reward | 42.4     |\n",
      "| steps                   | 56691    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6000     |\n",
      "| mean 100 episode reward | 42.3     |\n",
      "| steps                   | 57561    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6100     |\n",
      "| mean 100 episode reward | 41.2     |\n",
      "| steps                   | 58543    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6200     |\n",
      "| mean 100 episode reward | 42.1     |\n",
      "| steps                   | 59432    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6300     |\n",
      "| mean 100 episode reward | 41.8     |\n",
      "| steps                   | 60348    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6400     |\n",
      "| mean 100 episode reward | 38.1     |\n",
      "| steps                   | 61636    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6500     |\n",
      "| mean 100 episode reward | 34.8     |\n",
      "| steps                   | 63252    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6600     |\n",
      "| mean 100 episode reward | 40.1     |\n",
      "| steps                   | 64341    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6700     |\n",
      "| mean 100 episode reward | 41.7     |\n",
      "| steps                   | 65267    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6800     |\n",
      "| mean 100 episode reward | 33.4     |\n",
      "| steps                   | 67030    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 6900     |\n",
      "| mean 100 episode reward | 44.1     |\n",
      "| steps                   | 67720    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7000     |\n",
      "| mean 100 episode reward | 43.8     |\n",
      "| steps                   | 68441    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7100     |\n",
      "| mean 100 episode reward | 42.4     |\n",
      "| steps                   | 69297    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7200     |\n",
      "| mean 100 episode reward | 42.6     |\n",
      "| steps                   | 70135    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7300     |\n",
      "| mean 100 episode reward | 42.4     |\n",
      "| steps                   | 70991    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7400     |\n",
      "| mean 100 episode reward | 42.3     |\n",
      "| steps                   | 71859    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7500     |\n",
      "| mean 100 episode reward | 42.7     |\n",
      "| steps                   | 72690    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7600     |\n",
      "| mean 100 episode reward | 42.7     |\n",
      "| steps                   | 73518    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7700     |\n",
      "| mean 100 episode reward | 42.3     |\n",
      "| steps                   | 74390    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7800     |\n",
      "| mean 100 episode reward | 41.7     |\n",
      "| steps                   | 75318    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 7900     |\n",
      "| mean 100 episode reward | 41.2     |\n",
      "| steps                   | 76298    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8000     |\n",
      "| mean 100 episode reward | 43.5     |\n",
      "| steps                   | 77051    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8100     |\n",
      "| mean 100 episode reward | 44.4     |\n",
      "| steps                   | 77713    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8200     |\n",
      "| mean 100 episode reward | 42.2     |\n",
      "| steps                   | 78593    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8300     |\n",
      "| mean 100 episode reward | 42.4     |\n",
      "| steps                   | 79449    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8400     |\n",
      "| mean 100 episode reward | 40.7     |\n",
      "| steps                   | 80483    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8500     |\n",
      "| mean 100 episode reward | 44       |\n",
      "| steps                   | 81188    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8600     |\n",
      "| mean 100 episode reward | 44       |\n",
      "| steps                   | 81890    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8700     |\n",
      "| mean 100 episode reward | 41.2     |\n",
      "| steps                   | 82866    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8800     |\n",
      "| mean 100 episode reward | 38.6     |\n",
      "| steps                   | 84110    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 8900     |\n",
      "| mean 100 episode reward | 36.8     |\n",
      "| steps                   | 85532    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9000     |\n",
      "| mean 100 episode reward | 42.4     |\n",
      "| steps                   | 86395    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9100     |\n",
      "| mean 100 episode reward | 40.6     |\n",
      "| steps                   | 87432    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9200     |\n",
      "| mean 100 episode reward | 42       |\n",
      "| steps                   | 88327    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9300     |\n",
      "| mean 100 episode reward | 41.5     |\n",
      "| steps                   | 89279    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9400     |\n",
      "| mean 100 episode reward | 40.9     |\n",
      "| steps                   | 90292    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9500     |\n",
      "| mean 100 episode reward | 44.2     |\n",
      "| steps                   | 90968    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9600     |\n",
      "| mean 100 episode reward | 40.6     |\n",
      "| steps                   | 92010    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9700     |\n",
      "| mean 100 episode reward | 39.8     |\n",
      "| steps                   | 93134    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9800     |\n",
      "| mean 100 episode reward | 43.8     |\n",
      "| steps                   | 93852    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 9900     |\n",
      "| mean 100 episode reward | 34.4     |\n",
      "| steps                   | 95515    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 10000    |\n",
      "| mean 100 episode reward | 39.2     |\n",
      "| steps                   | 96692    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 10100    |\n",
      "| mean 100 episode reward | 42.4     |\n",
      "| steps                   | 97553    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 10200    |\n",
      "| mean 100 episode reward | 42.3     |\n",
      "| steps                   | 98423    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 10300    |\n",
      "| mean 100 episode reward | 41.9     |\n",
      "| steps                   | 99337    |\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines import DQN\n",
    "\n",
    "from stable_baselines.bench import Monitor\n",
    "\n",
    "log_dir = 'experiments'\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "model = DQN(MlpPolicy, env, verbose=1, prioritized_replay=True)\n",
    "#model = DQN(MlpPolicy, env, verbose=1, double_q=False, tensorboard_log=\"./sb_dqn_tensorboard/\")\n",
    "# model = DQN(MlpPolicy, env, verbose=1, tensorboard_log=\"./stable_baselines_DQN_tensorboard/\", prioritized_replay=True)\n",
    "\n",
    "model.learn(total_timesteps=100000) # not work: 10000, works: 100000\n",
    "model.save(\"deepq_\"+env_id)\n",
    "#model.save(\"deepq_\"+env_id+\"_new2\")\n",
    "\n",
    "#del model # remove to demonstrate saving and loading"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tensorboard --logdir \"./stable_baselines_DQN_tensorboard/\" --port=2000\n",
    "http://localhost:2000/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CECA903C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CECA903C8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CECA903C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CECA903C8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECA8F948>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECA8F948>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECA8F948>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECA8F948>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CEC178248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CEC178248>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CEC178248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CEC178248>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA67D3388>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA67D3388>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA67D3388>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA67D3388>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECA90408>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECA90408>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECA90408>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECA90408>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECB23388>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECB23388>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECB23388>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECB23388>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECB23B88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECB23B88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECB23B88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECB23B88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CECBDFF88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CECBDFF88>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CECBDFF88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CECBDFF88>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CEC196C08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CEC196C08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CEC196C08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CEC196C08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA67D3388>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA67D3388>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA67D3388>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA67D3388>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA67D3388>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA67D3388>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA67D3388>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CA67D3388>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECB05B88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECB05B88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECB05B88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECB05B88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9133788>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9133788>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9133788>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9133788>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9133788>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9133788>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9133788>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CC9133788>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CECB60408>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CECB60408>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CECB60408>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CECB60408>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECB4A8C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECB4A8C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECB4A8C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECB4A8C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECB16E48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECB16E48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECB16E48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECB16E48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECAEBD88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECAEBD88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECAEBD88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECAEBD88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECAA3248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECAA3248>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECAA3248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECAA3248>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECAA3248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECAA3248>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECAA3248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECAA3248>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECC1CBC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECC1CBC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECC1CBC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECC1CBC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CECB4AF08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CECB4AF08>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CECB4AF08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000016CECB4AF08>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECC1C148>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECC1C148>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECC1C148>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECC1C148>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECC59748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECC59748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECC59748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECC59748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECC1C148>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECC1C148>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECC1C148>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECC1C148>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECAAE908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECAAE908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECAAE908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECAAE908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECAC8088>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECAC8088>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECAC8088>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECAC8088>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECC59F08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECC59F08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECC59F08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000016CECC59F08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "model.q_values:  Tensor(\"deepq/model/add:0\", shape=(?, 3), dtype=float32)\n",
      "step_model.q_values:  Tensor(\"deepq/step_model/model/add:0\", shape=(?, 3), dtype=float32)\n",
      "double_policy.q_values:  Tensor(\"deepq/double_q/model/add:0\", shape=(?, 3), dtype=float32)\n",
      "target_policy.q_values:  Tensor(\"deepq/target_q_func/model/add:0\", shape=(?, 3), dtype=float32)\n",
      "q_func_vars:  [<tf.Variable 'deepq/model/action_value/fully_connected/weights:0' shape=(10, 64) dtype=float32_ref>, <tf.Variable 'deepq/model/action_value/fully_connected/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'deepq/model/action_value/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref>, <tf.Variable 'deepq/model/action_value/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'deepq/model/action_value/fully_connected_2/weights:0' shape=(64, 3) dtype=float32_ref>, <tf.Variable 'deepq/model/action_value/fully_connected_2/biases:0' shape=(3,) dtype=float32_ref>, <tf.Variable 'deepq/model/state_value/fully_connected/weights:0' shape=(10, 64) dtype=float32_ref>, <tf.Variable 'deepq/model/state_value/fully_connected/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'deepq/model/state_value/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref>, <tf.Variable 'deepq/model/state_value/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'deepq/model/state_value/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref>, <tf.Variable 'deepq/model/state_value/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref>]\n",
      "target_q_func_vars:  [<tf.Variable 'deepq/target_q_func/model/action_value/fully_connected/weights:0' shape=(10, 64) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/action_value/fully_connected/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/action_value/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/action_value/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/action_value/fully_connected_2/weights:0' shape=(64, 3) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/action_value/fully_connected_2/biases:0' shape=(3,) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/state_value/fully_connected/weights:0' shape=(10, 64) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/state_value/fully_connected/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/state_value/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/state_value/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/state_value/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref>, <tf.Variable 'deepq/target_q_func/model/state_value/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref>]\n",
      "EPISODE STARTS\n",
      "Episode #: 0, step: 0, reward:  -1\n",
      "Episode #: 0, step: 1, reward:  -1\n",
      "Episode #: 0, step: 2, reward:  -1\n",
      "Episode #: 0, step: 3, reward:  50\n",
      "Episode #: 0 finished after 3 timesteps\n",
      "\n",
      "EPISODE STARTS\n",
      "Episode #: 1, step: 0, reward:  -1\n",
      "Episode #: 1, step: 1, reward:  -1\n",
      "Episode #: 1, step: 2, reward:  -1\n",
      "Episode #: 1, step: 3, reward:  -1\n",
      "Episode #: 1, step: 4, reward:  -1\n",
      "Episode #: 1, step: 5, reward:  -1\n",
      "Episode #: 1, step: 6, reward:  -1\n",
      "Episode #: 1, step: 7, reward:  -1\n",
      "Episode #: 1, step: 8, reward:  50\n",
      "Episode #: 1 finished after 8 timesteps\n",
      "\n",
      "EPISODE STARTS\n",
      "Episode #: 2, step: 0, reward:  -1\n",
      "Episode #: 2, step: 1, reward:  50\n",
      "Episode #: 2 finished after 1 timesteps\n",
      "\n",
      "EPISODE STARTS\n",
      "Episode #: 3, step: 0, reward:  -1\n",
      "Episode #: 3, step: 1, reward:  -1\n",
      "Episode #: 3, step: 2, reward:  -1\n",
      "Episode #: 3, step: 3, reward:  -1\n",
      "Episode #: 3, step: 4, reward:  -1\n",
      "Episode #: 3, step: 5, reward:  -1\n",
      "Episode #: 3, step: 6, reward:  -1\n",
      "Episode #: 3, step: 7, reward:  -1\n",
      "Episode #: 3, step: 8, reward:  -1\n",
      "Episode #: 3, step: 9, reward:  -1\n",
      "Episode #: 3, step: 10, reward:  -1\n",
      "Episode #: 3, step: 11, reward:  -1\n",
      "Episode #: 3, step: 12, reward:  -1\n",
      "Episode #: 3, step: 13, reward:  -1\n",
      "Episode #: 3, step: 14, reward:  50\n",
      "Episode #: 3 finished after 14 timesteps\n",
      "\n",
      "EPISODE STARTS\n",
      "Episode #: 4, step: 0, reward:  -1\n",
      "Episode #: 4, step: 1, reward:  -1\n",
      "Episode #: 4, step: 2, reward:  -1\n",
      "Episode #: 4, step: 3, reward:  -1\n",
      "Episode #: 4, step: 4, reward:  -1\n",
      "Episode #: 4, step: 5, reward:  -1\n",
      "Episode #: 4, step: 6, reward:  -1\n",
      "Episode #: 4, step: 7, reward:  -1\n",
      "Episode #: 4, step: 8, reward:  50\n",
      "Episode #: 4 finished after 8 timesteps\n",
      "\n",
      "EPISODE STARTS\n",
      "Episode #: 5, step: 0, reward:  -1\n",
      "Episode #: 5, step: 1, reward:  -1\n",
      "Episode #: 5, step: 2, reward:  -1\n",
      "Episode #: 5, step: 3, reward:  -1\n",
      "Episode #: 5, step: 4, reward:  -1\n",
      "Episode #: 5, step: 5, reward:  -1\n",
      "Episode #: 5, step: 6, reward:  -1\n",
      "Episode #: 5, step: 7, reward:  -1\n",
      "Episode #: 5, step: 8, reward:  -1\n",
      "Episode #: 5, step: 9, reward:  -1\n",
      "Episode #: 5, step: 10, reward:  -1\n",
      "Episode #: 5, step: 11, reward:  50\n",
      "Episode #: 5 finished after 11 timesteps\n",
      "\n",
      "EPISODE STARTS\n",
      "Episode #: 6, step: 0, reward:  -1\n",
      "Episode #: 6, step: 1, reward:  50\n",
      "Episode #: 6 finished after 1 timesteps\n",
      "\n",
      "EPISODE STARTS\n",
      "Episode #: 7, step: 0, reward:  -1\n",
      "Episode #: 7, step: 1, reward:  -1\n",
      "Episode #: 7, step: 2, reward:  50\n",
      "Episode #: 7 finished after 2 timesteps\n",
      "\n",
      "EPISODE STARTS\n",
      "Episode #: 8, step: 0, reward:  -1\n",
      "Episode #: 8, step: 1, reward:  -1\n",
      "Episode #: 8, step: 2, reward:  -1\n",
      "Episode #: 8, step: 3, reward:  -1\n",
      "Episode #: 8, step: 4, reward:  -1\n",
      "Episode #: 8, step: 5, reward:  -1\n",
      "Episode #: 8, step: 6, reward:  -1\n",
      "Episode #: 8, step: 7, reward:  50\n",
      "Episode #: 8 finished after 7 timesteps\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE STARTS\n",
      "Episode #: 9, step: 0, reward:  -1\n",
      "Episode #: 9, step: 1, reward:  -1\n",
      "Episode #: 9, step: 2, reward:  -1\n",
      "Episode #: 9, step: 3, reward:  -1\n",
      "Episode #: 9, step: 4, reward:  -1\n",
      "Episode #: 9, step: 5, reward:  -1\n",
      "Episode #: 9, step: 6, reward:  50\n",
      "Episode #: 9 finished after 6 timesteps\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DQN.load(\"deepq_\"+env_id)\n",
    "# model = DQN.load(\"deepq_\"+env_id+\"_new2\")\n",
    "\n",
    "for i_episode in range(10):\n",
    "    print(\"EPISODE STARTS\")\n",
    "    obs = env.reset()\n",
    "    for i in range(100):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        print(\"Episode #: \" + str(i_episode) + \", step: \" + str(i) + \", reward: \", reward)\n",
    "        # End the episode if agent is dead\n",
    "        if done:\n",
    "            print(\"Episode #: \"+str(i_episode)+\" finished after \"+str(i)+\" timesteps\\n\")\n",
    "            time.sleep(1)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MlpPolicy in module stable_baselines.deepq.policies:\n",
      "\n",
      "class MlpPolicy(FeedForwardPolicy)\n",
      " |  MlpPolicy(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, obs_phs=None, dueling=True, **_kwargs)\n",
      " |  \n",
      " |  Policy object that implements DQN policy, using a MLP (2 layers of 64)\n",
      " |  \n",
      " |  :param sess: (TensorFlow session) The current TensorFlow session\n",
      " |  :param ob_space: (Gym Space) The observation space of the environment\n",
      " |  :param ac_space: (Gym Space) The action space of the environment\n",
      " |  :param n_env: (int) The number of environments to run\n",
      " |  :param n_steps: (int) The number of steps to run for each environment\n",
      " |  :param n_batch: (int) The number of batch to run (n_envs * n_steps)\n",
      " |  :param reuse: (bool) If the policy is reusable or not\n",
      " |  :param obs_phs: (TensorFlow Tensor, TensorFlow Tensor) a tuple containing an override for observation placeholder\n",
      " |      and the processed observation placeholder respectively\n",
      " |  :param dueling: (bool) if true double the output MLP to compute a baseline for action scores\n",
      " |  :param _kwargs: (dict) Extra keyword arguments for the nature CNN feature extraction\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MlpPolicy\n",
      " |      FeedForwardPolicy\n",
      " |      DQNPolicy\n",
      " |      stable_baselines.common.policies.BasePolicy\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, obs_phs=None, dueling=True, **_kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from FeedForwardPolicy:\n",
      " |  \n",
      " |  proba_step(self, obs, state=None, mask=None)\n",
      " |      Returns the action probability for a single step\n",
      " |      \n",
      " |      :param obs: (np.ndarray float or int) The current observation of the environment\n",
      " |      :param state: (np.ndarray float) The last states (used in recurrent policies)\n",
      " |      :param mask: (np.ndarray float) The last masks (used in recurrent policies)\n",
      " |      :return: (np.ndarray float) the action probability\n",
      " |  \n",
      " |  step(self, obs, state=None, mask=None, deterministic=True)\n",
      " |      Returns the q_values for a single step\n",
      " |      \n",
      " |      :param obs: (np.ndarray float or int) The current observation of the environment\n",
      " |      :param state: (np.ndarray float) The last states (used in recurrent policies)\n",
      " |      :param mask: (np.ndarray float) The last masks (used in recurrent policies)\n",
      " |      :param deterministic: (bool) Whether or not to return deterministic actions.\n",
      " |      :return: (np.ndarray int, np.ndarray float, np.ndarray float) actions, q_values, states\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from stable_baselines.common.policies.BasePolicy:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  action_ph\n",
      " |      tf.Tensor: placeholder for actions, shape (self.n_batch, ) + self.ac_space.shape.\n",
      " |  \n",
      " |  initial_state\n",
      " |      The initial state of the policy. For feedforward policies, None. For a recurrent policy,\n",
      " |      a NumPy array of shape (self.n_env, ) + state_shape.\n",
      " |  \n",
      " |  is_discrete\n",
      " |      bool: is action space discrete.\n",
      " |  \n",
      " |  obs_ph\n",
      " |      tf.Tensor: placeholder for observations, shape (self.n_batch, ) + self.ob_space.shape.\n",
      " |  \n",
      " |  processed_obs\n",
      " |      tf.Tensor: processed observations, shape (self.n_batch, ) + self.ob_space.shape.\n",
      " |      \n",
      " |      The form of processing depends on the type of the observation space, and the parameters\n",
      " |      whether scale is passed to the constructor; see observation_input for more information.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from stable_baselines.common.policies.BasePolicy:\n",
      " |  \n",
      " |  recurrent = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "help(MlpPolicy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MlpPolicy(FeedForwardPolicy)\n",
    "\n",
    "FeedForwardPolicy(DQNPolicy)\n",
    "    scope=\"model\"\n",
    "        scope=\"action_value\"\n",
    "        Building network: processed_obs -> Betwork -> action_scores (q_values)\n",
    "        if dueling:\n",
    "            scope=\"state_value\"\n",
    "            q_values = state_score + action_scores_centered\n",
    "    scope=\"output\"\n",
    "        policy_proba = softmax(q_values)\n",
    "        \n",
    "        \n",
    "\n",
    "DQNPolicy(BasePolicy)\n",
    "    dueling=True\n",
    "    \n",
    "BasePolicy(ABC)\n",
    "    scope=\"input\"\n",
    "        obs_ph, processed_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\anaconda3\\envs\\py3_7_sb\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DQN in module stable_baselines.deepq.dqn:\n",
      "\n",
      "class DQN(stable_baselines.common.base_class.OffPolicyRLModel)\n",
      " |  DQN(policy, env, gamma=0.99, learning_rate=0.0005, buffer_size=50000, exploration_fraction=0.1, exploration_final_eps=0.02, exploration_initial_eps=1.0, train_freq=1, batch_size=32, double_q=True, learning_starts=1000, target_network_update_freq=500, prioritized_replay=False, prioritized_replay_alpha=0.6, prioritized_replay_beta0=0.4, prioritized_replay_beta_iters=None, prioritized_replay_eps=1e-06, param_noise=False, n_cpu_tf_sess=None, verbose=0, tensorboard_log=None, _init_setup_model=True, policy_kwargs=None, full_tensorboard_log=False, seed=None)\n",
      " |  \n",
      " |  The DQN model class.\n",
      " |  DQN paper: https://arxiv.org/abs/1312.5602\n",
      " |  Dueling DQN: https://arxiv.org/abs/1511.06581\n",
      " |  Double-Q Learning: https://arxiv.org/abs/1509.06461\n",
      " |  Prioritized Experience Replay: https://arxiv.org/abs/1511.05952\n",
      " |  \n",
      " |  :param policy: (DQNPolicy or str) The policy model to use (MlpPolicy, CnnPolicy, LnMlpPolicy, ...)\n",
      " |  :param env: (Gym environment or str) The environment to learn from (if registered in Gym, can be str)\n",
      " |  :param gamma: (float) discount factor\n",
      " |  :param learning_rate: (float) learning rate for adam optimizer\n",
      " |  :param buffer_size: (int) size of the replay buffer\n",
      " |  :param exploration_fraction: (float) fraction of entire training period over which the exploration rate is\n",
      " |          annealed\n",
      " |  :param exploration_final_eps: (float) final value of random action probability\n",
      " |  :param exploration_initial_eps: (float) initial value of random action probability\n",
      " |  :param train_freq: (int) update the model every `train_freq` steps. set to None to disable printing\n",
      " |  :param batch_size: (int) size of a batched sampled from replay buffer for training\n",
      " |  :param double_q: (bool) Whether to enable Double-Q learning or not.\n",
      " |  :param learning_starts: (int) how many steps of the model to collect transitions for before learning starts\n",
      " |  :param target_network_update_freq: (int) update the target network every `target_network_update_freq` steps.\n",
      " |  :param prioritized_replay: (bool) if True prioritized replay buffer will be used.\n",
      " |  :param prioritized_replay_alpha: (float)alpha parameter for prioritized replay buffer.\n",
      " |      It determines how much prioritization is used, with alpha=0 corresponding to the uniform case.\n",
      " |  :param prioritized_replay_beta0: (float) initial value of beta for prioritized replay buffer\n",
      " |  :param prioritized_replay_beta_iters: (int) number of iterations over which beta will be annealed from initial\n",
      " |          value to 1.0. If set to None equals to max_timesteps.\n",
      " |  :param prioritized_replay_eps: (float) epsilon to add to the TD errors when updating priorities.\n",
      " |  :param param_noise: (bool) Whether or not to apply noise to the parameters of the policy.\n",
      " |  :param verbose: (int) the verbosity level: 0 none, 1 training information, 2 tensorflow debug\n",
      " |  :param tensorboard_log: (str) the log location for tensorboard (if None, no logging)\n",
      " |  :param _init_setup_model: (bool) Whether or not to build the network at the creation of the instance\n",
      " |  :param full_tensorboard_log: (bool) enable additional logging when using tensorboard\n",
      " |      WARNING: this logging can take a lot of space quickly\n",
      " |  :param seed: (int) Seed for the pseudo-random generators (python, numpy, tensorflow).\n",
      " |      If None (default), use random seed. Note that if you want completely deterministic\n",
      " |      results, you must set `n_cpu_tf_sess` to 1.\n",
      " |  :param n_cpu_tf_sess: (int) The number of threads for TensorFlow operations\n",
      " |      If None, the number of cpu of the current machine will be used.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DQN\n",
      " |      stable_baselines.common.base_class.OffPolicyRLModel\n",
      " |      stable_baselines.common.base_class.BaseRLModel\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, policy, env, gamma=0.99, learning_rate=0.0005, buffer_size=50000, exploration_fraction=0.1, exploration_final_eps=0.02, exploration_initial_eps=1.0, train_freq=1, batch_size=32, double_q=True, learning_starts=1000, target_network_update_freq=500, prioritized_replay=False, prioritized_replay_alpha=0.6, prioritized_replay_beta0=0.4, prioritized_replay_beta_iters=None, prioritized_replay_eps=1e-06, param_noise=False, n_cpu_tf_sess=None, verbose=0, tensorboard_log=None, _init_setup_model=True, policy_kwargs=None, full_tensorboard_log=False, seed=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  action_probability(self, observation, state=None, mask=None, actions=None, logp=False)\n",
      " |      If ``actions`` is ``None``, then get the model's action probability distribution from a given observation.\n",
      " |      \n",
      " |      Depending on the action space the output is:\n",
      " |          - Discrete: probability for each possible action\n",
      " |          - Box: mean and standard deviation of the action output\n",
      " |      \n",
      " |      However if ``actions`` is not ``None``, this function will return the probability that the given actions are\n",
      " |      taken with the given parameters (observation, state, ...) on this model. For discrete action spaces, it\n",
      " |      returns the probability mass; for continuous action spaces, the probability density. This is since the\n",
      " |      probability mass will always be zero in continuous spaces, see http://blog.christianperone.com/2019/01/\n",
      " |      for a good explanation\n",
      " |      \n",
      " |      :param observation: (np.ndarray) the input observation\n",
      " |      :param state: (np.ndarray) The last states (can be None, used in recurrent policies)\n",
      " |      :param mask: (np.ndarray) The last masks (can be None, used in recurrent policies)\n",
      " |      :param actions: (np.ndarray) (OPTIONAL) For calculating the likelihood that the given actions are chosen by\n",
      " |          the model for each of the given parameters. Must have the same number of actions and observations.\n",
      " |          (set to None to return the complete action probability distribution)\n",
      " |      :param logp: (bool) (OPTIONAL) When specified with actions, returns probability in log-space.\n",
      " |          This has no effect if actions is None.\n",
      " |      :return: (np.ndarray) the model's (log) action probability\n",
      " |  \n",
      " |  get_parameter_list(self)\n",
      " |      Get tensorflow Variables of model's parameters\n",
      " |      \n",
      " |      This includes all variables necessary for continuing training (saving / loading).\n",
      " |      \n",
      " |      :return: (list) List of tensorflow Variables\n",
      " |  \n",
      " |  learn(self, total_timesteps, callback=None, log_interval=100, tb_log_name='DQN', reset_num_timesteps=True, replay_wrapper=None)\n",
      " |      Return a trained model.\n",
      " |      \n",
      " |      :param total_timesteps: (int) The total number of samples to train on\n",
      " |      :param callback: (Union[callable, [callable], BaseCallback])\n",
      " |          function called at every steps with state of the algorithm.\n",
      " |          It takes the local and global variables. If it returns False, training is aborted.\n",
      " |          When the callback inherits from BaseCallback, you will have access\n",
      " |          to additional stages of the training (training start/end),\n",
      " |          please read the documentation for more details.\n",
      " |      :param log_interval: (int) The number of timesteps before logging.\n",
      " |      :param tb_log_name: (str) the name of the run for tensorboard log\n",
      " |      :param reset_num_timesteps: (bool) whether or not to reset the current timestep number (used in logging)\n",
      " |      :return: (BaseRLModel) the trained model\n",
      " |  \n",
      " |  predict(self, observation, state=None, mask=None, deterministic=True)\n",
      " |      Get the model's action from an observation\n",
      " |      \n",
      " |      :param observation: (np.ndarray) the input observation\n",
      " |      :param state: (np.ndarray) The last states (can be None, used in recurrent policies)\n",
      " |      :param mask: (np.ndarray) The last masks (can be None, used in recurrent policies)\n",
      " |      :param deterministic: (bool) Whether or not to return deterministic actions.\n",
      " |      :return: (np.ndarray, np.ndarray) the model's action and the next state (used in recurrent policies)\n",
      " |  \n",
      " |  save(self, save_path, cloudpickle=False)\n",
      " |      Save the current parameters to file\n",
      " |      \n",
      " |      :param save_path: (str or file-like) The save location\n",
      " |      :param cloudpickle: (bool) Use older cloudpickle format instead of zip-archives.\n",
      " |  \n",
      " |  setup_model(self)\n",
      " |      Create all the functions and tensorflow graphs necessary to train the model\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from stable_baselines.common.base_class.OffPolicyRLModel:\n",
      " |  \n",
      " |  is_using_her(self) -> bool\n",
      " |      Check if is using HER\n",
      " |      \n",
      " |      :return: (bool) Whether is using HER or not\n",
      " |  \n",
      " |  replay_buffer_add(self, obs_t, action, reward, obs_tp1, done, info)\n",
      " |      Add a new transition to the replay buffer\n",
      " |      \n",
      " |      :param obs_t: (np.ndarray) the last observation\n",
      " |      :param action: ([float]) the action\n",
      " |      :param reward: (float) the reward of the transition\n",
      " |      :param obs_tp1: (np.ndarray) the new observation\n",
      " |      :param done: (bool) is the episode done\n",
      " |      :param info: (dict) extra values used to compute the reward when using HER\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from stable_baselines.common.base_class.OffPolicyRLModel:\n",
      " |  \n",
      " |  load(load_path, env=None, custom_objects=None, **kwargs) from abc.ABCMeta\n",
      " |      Load the model from file\n",
      " |      \n",
      " |      :param load_path: (str or file-like) the saved parameter location\n",
      " |      :param env: (Gym Environment) the new environment to run the loaded model on\n",
      " |          (can be None if you only need prediction from a trained model)\n",
      " |      :param custom_objects: (dict) Dictionary of objects to replace\n",
      " |          upon loading. If a variable is present in this dictionary as a\n",
      " |          key, it will not be deserialized and the corresponding item\n",
      " |          will be used instead. Similar to custom_objects in\n",
      " |          `keras.models.load_model`. Useful when you have an object in\n",
      " |          file that can not be deserialized.\n",
      " |      :param kwargs: extra arguments to change the model when loading\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from stable_baselines.common.base_class.BaseRLModel:\n",
      " |  \n",
      " |  get_env(self)\n",
      " |      returns the current environment (can be None if not defined)\n",
      " |      \n",
      " |      :return: (Gym Environment) The current environment\n",
      " |  \n",
      " |  get_parameters(self)\n",
      " |      Get current model parameters as dictionary of variable name -> ndarray.\n",
      " |      \n",
      " |      :return: (OrderedDict) Dictionary of variable name -> ndarray of model's parameters.\n",
      " |  \n",
      " |  get_vec_normalize_env(self) -> Union[stable_baselines.common.vec_env.vec_normalize.VecNormalize, NoneType]\n",
      " |      Return the ``VecNormalize`` wrapper of the training env\n",
      " |      if it exists.\n",
      " |      \n",
      " |      :return: Optional[VecNormalize] The ``VecNormalize`` env.\n",
      " |  \n",
      " |  load_parameters(self, load_path_or_dict, exact_match=True)\n",
      " |      Load model parameters from a file or a dictionary\n",
      " |      \n",
      " |      Dictionary keys should be tensorflow variable names, which can be obtained\n",
      " |      with ``get_parameters`` function. If ``exact_match`` is True, dictionary\n",
      " |      should contain keys for all model's parameters, otherwise RunTimeError\n",
      " |      is raised. If False, only variables included in the dictionary will be updated.\n",
      " |      \n",
      " |      This does not load agent's hyper-parameters.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          This function does not update trainer/optimizer variables (e.g. momentum).\n",
      " |          As such training after using this function may lead to less-than-optimal results.\n",
      " |      \n",
      " |      :param load_path_or_dict: (str or file-like or dict) Save parameter location\n",
      " |          or dict of parameters as variable.name -> ndarrays to be loaded.\n",
      " |      :param exact_match: (bool) If True, expects load dictionary to contain keys for\n",
      " |          all variables in the model. If False, loads parameters only for variables\n",
      " |          mentioned in the dictionary. Defaults to True.\n",
      " |  \n",
      " |  pretrain(self, dataset, n_epochs=10, learning_rate=0.0001, adam_epsilon=1e-08, val_interval=None)\n",
      " |      Pretrain a model using behavior cloning:\n",
      " |      supervised learning given an expert dataset.\n",
      " |      \n",
      " |      NOTE: only Box and Discrete spaces are supported for now.\n",
      " |      \n",
      " |      :param dataset: (ExpertDataset) Dataset manager\n",
      " |      :param n_epochs: (int) Number of iterations on the training set\n",
      " |      :param learning_rate: (float) Learning rate\n",
      " |      :param adam_epsilon: (float) the epsilon value for the adam optimizer\n",
      " |      :param val_interval: (int) Report training and validation losses every n epochs.\n",
      " |          By default, every 10th of the maximum number of epochs.\n",
      " |      :return: (BaseRLModel) the pretrained model\n",
      " |  \n",
      " |  set_env(self, env)\n",
      " |      Checks the validity of the environment, and if it is coherent, set it as the current environment.\n",
      " |      \n",
      " |      :param env: (Gym Environment) The environment for learning a policy\n",
      " |  \n",
      " |  set_random_seed(self, seed: Union[int, NoneType]) -> None\n",
      " |      :param seed: (Optional[int]) Seed for the pseudo-random generators. If None,\n",
      " |          do not change the seeds.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from stable_baselines.common.base_class.BaseRLModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines import DQN\n",
    "help(DQN)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = DQN(MlpPolicy, env, verbose=1)\n",
    "============================================\n",
    "DQN(OffPolicyRLModel).__init__(policy, env, gamma=0.99, learning_rate=5e-4, buffer_size=50000, exploration_fraction=0.1,\n",
    "         exploration_final_eps=0.02, exploration_initial_eps=1.0, train_freq=1, batch_size=32, double_q=True,\n",
    "         learning_starts=1000, target_network_update_freq=500, prioritized_replay=False,\n",
    "         prioritized_replay_alpha=0.6, prioritized_replay_beta0=0.4, prioritized_replay_beta_iters=None,\n",
    "         prioritized_replay_eps=1e-6, param_noise=False, n_cpu_tf_sess=None, verbose=0, tensorboard_log=None,\n",
    "         _init_setup_model=True, policy_kwargs=None, full_tensorboard_log=False, seed=None):\n",
    "         \n",
    "    build_train(q_func, ob_space, ac_space, optimizer, sess, grad_norm_clipping=None, gamma=1.0, double_q=True,\n",
    "                scope=\"deepq\", reuse=None, param_noise=False, param_noise_filter_func=None, full_tensorboard_log=False):\n",
    "        scope=\"input\"\n",
    "            stochastic_ph, update_eps_ph\n",
    "        scope=\"deepq\"\n",
    "            act_f = build_act() or build_act_with_param_noise() - network with added noise:\n",
    "                Not sure, but:\n",
    "                Build a network for training - see FeedForwardPolicy(DQNPolicy)\n",
    "                Take random or determinism action based on epsilon\n",
    "                Find how epsilon gets updated??\n",
    "            scope=\"step_model\"\n",
    "                # q network evaluation\n",
    "                Build a network for testing (taking actions) - see FeedForwardPolicy(DQNPolicy)\n",
    "            scope=\"target_q_func\"\n",
    "                # target q network evaluation\n",
    "                Build target_policy - see FeedForwardPolicy(DQNPolicy)\n",
    "                Without obs_phs\n",
    "            if double_q:\n",
    "                scope=\"double_q\"\n",
    "                    # compute estimate of best possible value starting from state at t + 1\n",
    "                    Build double_policy - see FeedForwardPolicy(DQNPolicy)\n",
    "                    Without obs_phs\n",
    "         scope=\"loss\"\n",
    "             act_t_ph, rew_t_ph, done_mask_ph, importance_weights_ph\n",
    "             q_t_selected: q scores for actions which we know were selected in the given state (given by step_model)\n",
    "             \n",
    "             if double_q:\n",
    "                 # compute estimate of best possible value starting from state at t + 1\n",
    "                 q_tp1_best: q value of target_policy of best action of double_policy\n",
    "             else:\n",
    "                 q_tp1_best: q value of target_policy of best action of target_policy\n",
    "             \n",
    "             q_tp1_best_masked = (1.0 - done_mask_ph) * q_tp1_best # When episode end, don't consider reward of next state\n",
    "             \n",
    "             # compute RHS of bellman equation\n",
    "             q_t_selected_target = rew_t_ph + gamma * q_tp1_best_masked\n",
    "             \n",
    "             # compute the error (potentially clipped)\n",
    "             td_error = q_t_selected - tf.stop_gradient(q_t_selected_target) # I think, this will not optimize target_policy\n",
    "             errors = tf_util.huber_loss(td_error)\n",
    "             weighted_error = importance_weights_ph * errors\n",
    "             \n",
    "             # update_target_fn will be called periodically to copy Q network to target Q network\n",
    "             update_target_expr: q_func_vars (model) ==> target_q_func_vars (target_policy)\n",
    "             \n",
    "             # compute optimization op (potentially with gradient clipping)\n",
    "             gradients = optimizer.compute_gradients(weighted_error, var_list=q_func_vars)\n",
    "         \n",
    "         optimize_expr = optimizer.apply_gradients(gradients)\n",
    "         \n",
    "         train = run session for td_error, optimize_expr\n",
    "         update_target = run session for update_target_expr\n",
    "         \n",
    "         return act_f, train, update_target, step_model\n",
    "         \n",
    "    proba_step = step_model.proba_step # gives probability of each action\n",
    "    params = tf_util.get_trainable_vars(\"deepq\") # I think, this does not return target_policy variables\n",
    "\n",
    "OffPolicyRLModel(BaseRLModel)\n",
    "\n",
    "BaseRLModel(ABC)\n",
    "    Vectorize environment\n",
    "    self.policy\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.learn(total_timesteps=10000)\n",
    "======================================\n",
    "\n",
    "learn(total_timesteps, callback=None, log_interval=100, tb_log_name=\"DQN\", reset_num_timesteps=True, replay_wrapper=None):\n",
    "    \n",
    "    if self.prioritized_replay:\n",
    "        self.replay_buffer = PrioritizedReplayBuffer(self.buffer_size, alpha=self.prioritized_replay_alpha)\n",
    "        self.beta_schedule = LinearSchedule(prioritized_replay_beta_iters, initial_p=self.prioritized_replay_beta0,\n",
    "                                            final_p=1.0)\n",
    "    else:\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size)\n",
    "        self.beta_schedule = None\n",
    "    \n",
    "    self.exploration = LinearSchedule(schedule_timesteps=int(self.exploration_fraction * total_timesteps),\n",
    "                                              initial_p=self.exploration_initial_eps,\n",
    "                                              final_p=self.exploration_final_eps)\n",
    "    \n",
    "    \n",
    "    for _ in range(total_timesteps):\n",
    "        \n",
    "        action = self.act(np.array(obs)[None], update_eps=update_eps, **kwargs)[0] # run on \"model\"\n",
    "        new_obs, rew, done, info = self.env.step(env_action)\n",
    "        \n",
    "        # Store transition in the replay buffer\n",
    "        self.replay_buffer_add(obs_, action, reward_, new_obs_, done, info)\n",
    "        \n",
    "        if some condition to optimize:\n",
    "            \n",
    "            if self.prioritized_replay:\n",
    "                experience = self.replay_buffer.sample(self.batch_size, beta=self.beta_schedule.value(self.num_timesteps),\n",
    "                                                       env=self._vec_normalize_env)\n",
    "                (obses_t, actions, rewards, obses_tp1, dones, weights, batch_idxes) = experience\n",
    "            else:\n",
    "                obses_t, actions, rewards, obses_tp1, dones = self.replay_buffer.sample(self.batch_size,\n",
    "                                                                                        env=self._vec_normalize_env)\n",
    "                weights, batch_idxes = np.ones_like(rewards), None\n",
    "            \n",
    "            \n",
    "            _, td_errors = self._train_step(obses_t, actions, rewards, obses_tp1, obses_tp1, dones, weights, sess=self.sess)\n",
    "                                                \n",
    "            if self.prioritized_replay:\n",
    "                new_priorities = np.abs(td_errors) + self.prioritized_replay_eps\n",
    "                self.replay_buffer.update_priorities(batch_idxes, new_priorities) # Update priorities based on td_errors\n",
    "            \n",
    "        if some condition to update target_network:\n",
    "            # Update target network periodically.\n",
    "            self.update_target(sess=self.sess)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "action, _states = model.predict(obs)\n",
    "===============================================\n",
    "predict(self, observation, state=None, mask=None, deterministic=True):\n",
    "    \n",
    "    actions, _, _ = self.step_model.step(observation, deterministic=deterministic)\n",
    "    \n",
    "    return actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of SB's DQN\n",
    "\n",
    "DQN Tricks:\n",
    "- Dueling DQN\n",
    "- Double-Q Learning\n",
    "- Prioritized Experience Replay\n",
    "\n",
    "There are 4 networks:\n",
    "- model: It's behavior/Q-network, used for learning/optimization\n",
    "- step_model: It's input is same as model (obs_phs), and its also used for for learning/optimization. But, its also used when we have to take action in the env.\n",
    "- double_q: its a separate network like target_network used to compute td_errors\n",
    "- target_network: periodically Q-network is copied to target_network\n",
    "\n",
    "model.q_values:  Tensor(\"deepq/model/add:0\", shape=(?, 3), dtype=float32)\n",
    "step_model.q_values:  Tensor(\"deepq/step_model/model/add:0\", shape=(?, 3), dtype=float32)\n",
    "double_policy.q_values:  Tensor(\"deepq/double_q/model/add:0\", shape=(?, 3), dtype=float32)\n",
    "target_policy.q_values:  Tensor(\"deepq/target_q_func/model/add:0\", shape=(?, 3), dtype=float32)\n",
    "\n",
    "Implementation Details:\n",
    "- A network is created by MlpPolicy, which has \"model\" scope\n",
    "    - model is used to optimize\n",
    "- This model is copied to 3 models under different scopes: step_model, double_q, target_q_func\n",
    "    - step_model is uded to take actions or predict function\n",
    "    - target_q_func is computed the error\n",
    "- Input (obs_phs=obs_phs) is connected to model and step_model \n",
    "- Initially output of all network is same:\n",
    "```\n",
    "num_timesteps:  0\n",
    "[[-0.41003048 -0.33983934  3.8062277 ]] - model q_values\n",
    "[[-0.41003048 -0.33983934  3.8062277 ]] - step_model q_values\n",
    "[[-0.41003048 -0.33983934  3.8062277 ]] - double_policy q_values\n",
    "[[-0.41003048 -0.33983934  3.8062277 ]] - target_policy q_values\n",
    "```\n",
    "- When training starts model, step_model, double_q is updated:\n",
    "```\n",
    "num_timesteps:  1001\n",
    "[[2.5672882 2.647919  6.15353  ]] - model q_values\n",
    "[[2.5672882 2.647919  6.15353  ]] - step_model q_values\n",
    "[[2.5672882 2.647919  6.15353  ]] - double_policy q_values\n",
    "[[1.8600085 2.0999918 6.044751 ]] - target_policy q_values\n",
    "```\n",
    "- For training, experience data is sampled using priorities based on td_errors\n",
    "- After every 500 steps, model parameters is copied to target_policy, so their output becomes same:\n",
    "```\n",
    "num_timesteps:  1500\n",
    "[[-0.13262868  0.72109544  0.81133664]] - model q_values\n",
    "[[-0.13262868  0.72109544  0.81133664]] - step_model q_values\n",
    "[[-0.13262868  0.72109544  0.81133664]] - double_policy q_values\n",
    "[[-0.13262868  0.72109544  0.81133664]] - target_policy q_values\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
