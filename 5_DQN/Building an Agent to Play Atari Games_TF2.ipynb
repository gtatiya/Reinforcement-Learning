{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Agent to Play Atari games using Deep Q Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First we import all the necessary libraries </font> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\anaconda3\\envs\\py3_7\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "print(tf.__version__)\n",
    "\n",
    "# from tensorflow.contrib.layers import flatten, conv2d, fully_connected\n",
    "from tensorflow.compat.v1.layers import flatten, conv2d, dense\n",
    "\n",
    "from collections import deque, Counter\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we define a function called preprocess_observation for preprocessing our input game screen. We reduce the image size\n",
    "and convert the image into greyscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = np.array([210, 164, 74]).mean()\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "\n",
    "    # Crop and resize the image\n",
    "    img = obs[1:176:2, ::2]\n",
    "\n",
    "    # Convert the image to greyscale\n",
    "    img = img.mean(axis=2)\n",
    "\n",
    "    # Improve image contrast\n",
    "    img[img==color] = 0\n",
    "\n",
    "    # Next we normalize the image from -1 to +1\n",
    "    img = (img - 128) / 128 - 1\n",
    "\n",
    "    return img.reshape(88,80,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let us initialize our gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_outputs:  4\n",
      "action_space: Discrete(4)\n",
      "observation_space: Box(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Breakout-v0\") #gym.make(\"MsPacman-v0\")\n",
    "n_outputs = env.action_space.n\n",
    "\n",
    "print(\"n_outputs: \", n_outputs)\n",
    "print(\"action_space:\", env.action_space)\n",
    "print(\"observation_space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original\n"
     ]
    }
   ],
   "source": [
    "filename = \"original-512\" #\"my_fix-512\"\n",
    "\n",
    "if filename.startswith('original'):\n",
    "    print(\"original\")\n",
    "else:\n",
    "    print(\"my_fix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Okay, Now we define a function called q_network for building our Q network. We input the game state\n",
    "to the Q network and get the Q values for all the actions in that state. <br><br>\n",
    "We build Q network with three convolutional layers with same padding followed by a fully connected layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def q_network(X, name_scope):\n",
    "    \n",
    "    # Initialize layers\n",
    "    # initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "    initializer = tf.keras.initializers.VarianceScaling(scale=2.0)\n",
    "\n",
    "    with tf.variable_scope(name_scope) as scope: \n",
    "\n",
    "        # initialize the convolutional layers\n",
    "        #layer_1 = conv2d(X, num_outputs=32, kernel_size=(8,8), stride=4, padding='SAME', weights_initializer=initializer) \n",
    "        layer_1 = conv2d(X, filters=32, kernel_size=(8,8), strides=4, padding='SAME')\n",
    "        tf.summary.histogram('layer_1',layer_1)\n",
    "        \n",
    "        #layer_2 = conv2d(layer_1, num_outputs=64, kernel_size=(4,4), stride=2, padding='SAME', weights_initializer=initializer)\n",
    "        layer_2 = conv2d(layer_1, filters=64, kernel_size=(4,4), strides=2, padding='SAME')\n",
    "        tf.summary.histogram('layer_2',layer_2)\n",
    "        \n",
    "        #layer_3 = conv2d(layer_2, num_outputs=64, kernel_size=(3,3), stride=1, padding='SAME', weights_initializer=initializer)\n",
    "        layer_3 = conv2d(layer_2, filters=64, kernel_size=(3,3), strides=1, padding='SAME')\n",
    "        tf.summary.histogram('layer_3',layer_3)\n",
    "        \n",
    "        # Flatten the result of layer_3 before feeding to the fully connected layer\n",
    "        flat = flatten(layer_3)\n",
    "\n",
    "        #fc = fully_connected(flat, num_outputs=128, weights_initializer=initializer)\n",
    "        #fc = dense(flat, units=128)\n",
    "        fc = dense(flat, units=512) #GT\n",
    "        tf.summary.histogram('fc',fc)\n",
    "        \n",
    "        #output = fully_connected(fc, num_outputs=n_outputs, activation_fn=None, weights_initializer=initializer)\n",
    "        output = dense(fc, units=n_outputs, activation=None)\n",
    "        tf.summary.histogram('output',output)\n",
    "        \n",
    "        # Vars will store the parameters of the network such as weights\n",
    "        vars = {v.name[len(scope.name):]: v for v in tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)} \n",
    "        return vars, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a function called epsilon_greedy for performing epsilon greedy policy. In epsilon greedy policy we either select the best action with probability 1 - epsilon or a random action with\n",
    "probability epsilon.\n",
    "\n",
    "We use decaying epsilon greedy policy where value of epsilon will be decaying over time as we don't want to explore\n",
    "forever. So over time our policy will be exploiting only good actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.5\n",
    "eps_min = 0.05\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 500000\n",
    "\n",
    "def epsilon_greedy(action, step):\n",
    "    p = np.random.random(1).squeeze()\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we initialize our experience replay buffer of length 20000 which holds the experience.\n",
    "\n",
    "We store all the agent's experience i.e (state, action, rewards) in the experience replay buffer\n",
    "and  we sample from this minibatch of experience for training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_len = 20000\n",
    "exp_buffer = deque(maxlen=buffer_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function called sample_memories for sampling experiences from the memory. Batch size is the number of experience sampled\n",
    "from the memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_memories(batch_size):\n",
    "    perm_batch = np.random.permutation(len(exp_buffer))[:batch_size]\n",
    "    mem = np.array(exp_buffer)[perm_batch]\n",
    "    return mem[:,0], mem[:,1], mem[:,2], mem[:,3], mem[:,4] # obs, action, next_obs, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our network hyperparameters,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000 #800\n",
    "batch_size = 48\n",
    "#input_shape = (None, 88, 80, 1)\n",
    "#input_shape = (None, 84, 84, 1) #GT\n",
    "input_shape = (None, 84, 84, 4) #GT\n",
    "learning_rate = 0.001\n",
    "#X_shape = (None, 88, 80, 1)\n",
    "#X_shape = (None, 84, 84, 1) #GT\n",
    "X_shape = (None, 84, 84, 4) #GT\n",
    "discount_factor = 0.97\n",
    "\n",
    "global_step = 0\n",
    "copy_steps = 100\n",
    "steps_train = 4\n",
    "start_steps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = filename\n",
    "\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Now we define the placeholder for our input i.e game state\n",
    "X = tf.placeholder(tf.float32, shape=X_shape)\n",
    "\n",
    "# we define a boolean called in_training_model to toggle the training\n",
    "in_training_mode = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now let us build our primary and target Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-9a5849e3d772>:13: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From c:\\anaconda3\\envs\\py3_7\\lib\\site-packages\\tensorflow_core\\python\\layers\\convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-9a5849e3d772>:25: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-9a5849e3d772>:29: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    }
   ],
   "source": [
    "# we build our Q network, which takes the input X and generates Q values for all the actions in the state\n",
    "mainQ, mainQ_outputs = q_network(X, 'mainQ')\n",
    "\n",
    "# similarly we build our target Q network\n",
    "targetQ, targetQ_outputs = q_network(X, 'targetQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-9028b1501bc8>:4: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Q_action:  Tensor(\"Sum:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# define the placeholder for our action values\n",
    "X_action = tf.placeholder(tf.int32, shape=(None,))\n",
    "if filename.startswith('original'):\n",
    "    Q_action = tf.reduce_sum(targetQ_outputs * tf.one_hot(X_action, n_outputs), axis=-1, keep_dims=True)\n",
    "else:\n",
    "    Q_action = tf.reduce_sum(mainQ_outputs * tf.one_hot(X_action, n_outputs), axis=-1, keep_dims=True) #GT\n",
    "\n",
    "print(\"Q_action: \", Q_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the primary Q network parameters to the target  Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filename.startswith('original'):\n",
    "    copy_op = [tf.assign(main_name, targetQ[var_name]) for var_name, main_name in mainQ.items()]\n",
    "    copy_target_to_main = tf.group(*copy_op)\n",
    "else:\n",
    "    #GT\n",
    "    #tf.compat.v1.assign(ref, value): outputs a Tensor that holds the new value of ref after the value has been assigned\n",
    "    copy_op = [tf.assign(target_name, mainQ[var_name]) for var_name, target_name in targetQ.items()]\n",
    "    copy_target_to_main = tf.group(*copy_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and optimize loss using gradient descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a placeholder for our output i.e action\n",
    "y = tf.placeholder(tf.float32, shape=(None,1))\n",
    "\n",
    "# now we calculate the loss which is the difference between actual value and predicted value\n",
    "loss = tf.reduce_mean(tf.square(y - Q_action))\n",
    "\n",
    "# we use adam optimizer for minimizing the loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "loss_summary = tf.summary.scalar('LOSS', loss)\n",
    "merge_summary = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "checkpoint_path = logdir\n",
    "saver = tf.train.Saver()\n",
    "# latest_checkpoint = checkpoint_path #GT: load model\n",
    "# if latest_checkpoint:\n",
    "#     print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "#     saver.restore(sess, latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation_gt(obs):\n",
    "\n",
    "    # Crop\n",
    "    img = obs[34:34+160]\n",
    "    \n",
    "    # Convert the image to greyscale\n",
    "    img = img.mean(axis=2)\n",
    "\n",
    "    # Resize\n",
    "    img = resize(img, (84, 84))\n",
    "\n",
    "    return img.reshape(84, 84, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nobs = env.reset()\\n\\nprint(\"obs: \", obs.shape)\\nplt.imshow(obs)\\nplt.colorbar()\\nplt.show()\\n\\n# get the preprocessed game screen\\n#obs = preprocess_observation(obs)\\nobs = preprocess_observation_gt(obs)\\n\\nprint(\"obs: \", obs.shape)\\nplt.imshow(obs.reshape((obs.shape[0], obs.shape[1])))\\nplt.colorbar()\\nplt.show()\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Testing....\n",
    "\n",
    "\"\"\"\n",
    "obs = env.reset()\n",
    "\n",
    "print(\"obs: \", obs.shape)\n",
    "plt.imshow(obs)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# get the preprocessed game screen\n",
    "#obs = preprocess_observation(obs)\n",
    "obs = preprocess_observation_gt(obs)\n",
    "\n",
    "print(\"obs: \", obs.shape)\n",
    "plt.imshow(obs.reshape((obs.shape[0], obs.shape[1])))\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari images. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntf.reset_default_graph()\\nglobal_step = tf.Variable(0, name=\"global_step\", trainable=False)\\n\\nsp = StateProcessor()\\n\\nwith tf.Session() as sess:\\n    sess.run(tf.global_variables_initializer())\\n    \\n    # Example observation batch\\n    observation = env.reset()\\n    \\n    print(\"observation: \", observation.shape)\\n    plt.imshow(observation)\\n    plt.colorbar()\\n    plt.show()\\n    \\n    observation_p = sp.process(sess, observation)\\n    print(\"observation_p: \", observation_p.shape)\\n    plt.imshow(observation_p)\\n    plt.colorbar()\\n    plt.show()\\n    \\n    ###\\n    state = env.reset()\\n    state = state_processor.process(sess, state)\\n    state = np.stack([state] * 4, axis=2)\\n    next_state = state_processor.process(sess, next_state)\\n    next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\\n    ###\\n    \\n    observation = np.stack([observation_p] * 4, axis=2)\\n    print(\"observation: \", observation.shape)\\n    plt.imshow(observation)\\n    plt.colorbar()\\n    plt.show()\\n    \\n    observations = np.array([observation] * 2)\\n    print(\"observations: \", observations.shape)\\n    plt.imshow(observations[0])\\n    plt.colorbar()\\n    plt.show()\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Testing....\n",
    "\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example observation batch\n",
    "    observation = env.reset()\n",
    "    \n",
    "    print(\"observation: \", observation.shape)\n",
    "    plt.imshow(observation)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    observation_p = sp.process(sess, observation)\n",
    "    print(\"observation_p: \", observation_p.shape)\n",
    "    plt.imshow(observation_p)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    ###\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    next_state = state_processor.process(sess, next_state)\n",
    "    next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "    ###\n",
    "    \n",
    "    observation = np.stack([observation_p] * 4, axis=2)\n",
    "    print(\"observation: \", observation.shape)\n",
    "    plt.imshow(observation)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    observations = np.array([observation] * 2)\n",
    "    print(\"observations: \", observations.shape)\n",
    "    plt.imshow(observations[0])\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we start the tensorflow session and run the model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0 Epoch: 378 Reward: 4.0\n",
      "i: 1 Epoch: 355 Reward: 3.0\n",
      "i: 2 Epoch: 317 Reward: 3.0\n",
      "i: 3 Epoch: 175 Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "state_processor = StateProcessor()\n",
    "\n",
    "with open(filename+'.csv','w') as f:\n",
    "    writer = csv.writer(f, lineterminator=\"\\n\")\n",
    "    writer.writerow(['episode', 'reward'])\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    all_episodic_reward = []\n",
    "    # for each episode\n",
    "    for i in range(num_episodes):\n",
    "        \n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), logdir+os.sep+checkpoint_path)\n",
    "        \n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        epoch = 0\n",
    "        episodic_reward = 0\n",
    "        actions_counter = Counter() \n",
    "        episodic_loss = []\n",
    "        \n",
    "        obs = state_processor.process(sess, obs) #GT\n",
    "        obs = np.stack([obs] * 4, axis=2) #GT\n",
    "\n",
    "        # while the state is not the terminal state\n",
    "        while not done:\n",
    "\n",
    "            #env.render()\n",
    "        \n",
    "            # get the preprocessed game screen\n",
    "            #obs = preprocess_observation(obs)\n",
    "            #obs = preprocess_observation_gt(obs) #GT\n",
    "            \n",
    "            #plt.imshow(obs.reshape((obs.shape[0], obs.shape[1])))\n",
    "            #plt.colorbar()\n",
    "            #plt.show()\n",
    "\n",
    "            # feed the game screen and get the Q values for each action\n",
    "            actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode:False})\n",
    "\n",
    "            # get the action\n",
    "            action = np.argmax(actions, axis=-1)\n",
    "            actions_counter[str(action)] += 1 \n",
    "\n",
    "            # select the action using epsilon greedy policy\n",
    "            action = epsilon_greedy(action, global_step)\n",
    "            \n",
    "            # now perform the action and move to the next state, next_obs, receive reward\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Store this transistion as an experience in the replay buffer\n",
    "            #exp_buffer.append([obs, action, preprocess_observation(next_obs), reward, done])\n",
    "            #exp_buffer.append([obs, action, preprocess_observation_gt(next_obs), reward, done]) #GT\n",
    "            next_obs = state_processor.process(sess, next_obs) #GT\n",
    "            next_obs = np.stack([next_obs] * 4, axis=2) #GT\n",
    "            exp_buffer.append([obs, action, next_obs, reward, done]) #GT\n",
    "            \n",
    "            # After certain steps, we train our Q network with samples from the experience replay buffer\n",
    "            if global_step % steps_train == 0 and global_step > start_steps:\n",
    "                #print('i:', i, 'Learn: ', global_step, start_steps, steps_train)\n",
    "                \n",
    "                # sample experience\n",
    "                o_obs, o_act, o_next_obs, o_rew, o_done = sample_memories(batch_size)\n",
    "\n",
    "                # states\n",
    "                o_obs = [x for x in o_obs]\n",
    "\n",
    "                # next states\n",
    "                o_next_obs = [x for x in o_next_obs]\n",
    "\n",
    "                # next actions\n",
    "                if filename.startswith('original'):\n",
    "                    next_act = mainQ_outputs.eval(feed_dict={X:o_next_obs, in_training_mode:False})                \n",
    "                else:\n",
    "                    next_act = targetQ_outputs.eval(feed_dict={X:o_next_obs, in_training_mode:False})  #GT\n",
    "    \n",
    "                # reward\n",
    "                y_batch = o_rew + discount_factor * np.max(next_act, axis=-1) * (1-o_done) \n",
    "\n",
    "                # merge all summaries and write to the file\n",
    "                mrg_summary = merge_summary.eval(feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:False})\n",
    "                file_writer.add_summary(mrg_summary, global_step)\n",
    "\n",
    "                # now we train the network and calculate loss\n",
    "                train_loss, _ = sess.run([loss, training_op], feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:True})\n",
    "                episodic_loss.append(train_loss)\n",
    "            \n",
    "            # after some interval we copy our main Q network weights to target Q network\n",
    "            if (global_step+1) % copy_steps == 0 and global_step > start_steps:\n",
    "                #print('i:', i, 'Replace: ', global_step+1, start_steps, copy_steps)\n",
    "                copy_target_to_main.run()\n",
    "                \n",
    "            obs = next_obs\n",
    "            epoch += 1\n",
    "            global_step += 1\n",
    "            episodic_reward += reward\n",
    "        \n",
    "        all_episodic_reward.append(episodic_reward)\n",
    "        print('i:', i, 'Epoch:', epoch, 'Reward:', episodic_reward)\n",
    "        \n",
    "        plt.plot(all_episodic_reward)\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.xlabel('training steps')\n",
    "        plt.title(filename, fontsize=20)\n",
    "        plt.savefig(filename+\".png\", bbox_inches='tight', dpi=100)\n",
    "        plt.close()\n",
    "        \n",
    "        with open(filename+'.csv', 'a') as f: # append to the file created\n",
    "            writer = csv.writer(f, lineterminator=\"\\n\")\n",
    "            writer.writerow([i+1, episodic_reward])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
